

// const express = require('express');
// const cors = require('cors');
// const Pusher = require('pusher');
// const WebSocket = require('ws');
// require('dotenv').config();

// const app = express();

// // Initialize Pusher
// const pusher = new Pusher({
//   appId: process.env.PUSHER_APP_ID,
//   key: process.env.PUSHER_KEY,
//   secret: process.env.PUSHER_SECRET,
//   cluster: process.env.PUSHER_CLUSTER,
//   useTLS: true
// });

// console.log('ðŸ”§ Pusher initialized with cluster:', process.env.PUSHER_CLUSTER);

// // Middleware
// app.use(cors({
//   origin: '*',
//   methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
//   allowedHeaders: ['Origin', 'X-Requested-With', 'Content-Type', 'Accept', 'Authorization'],
// }));
// app.use(express.json({ limit: '10mb' }));
// app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
// console.log('ðŸ”‘ OpenAI API Key present:', !!OPENAI_API_KEY);

// // Store active OpenAI connections
// const openaiConnections = new Map();
// // Store audio responses per session
// const audioResponses = new Map();

// // Routes
// app.get('/', (req, res) => {
//   console.log('ðŸ“ Root endpoint hit');
//   res.json({ 
//     message: 'Zoom Voice Bot API',
//     status: 'running',
//     endpoints: {
//       health: '/api/health',
//       connect: '/api/connect',
//       sendAudio: '/api/send-audio',
//       getAudio: '/api/get-audio/:sessionId'
//     }
//   });
// });

// app.get('/api/health', (req, res) => {
//   console.log('ðŸ“ Health check endpoint hit');
//   res.json({ 
//     status: 'ok', 
//     connections: openaiConnections.size,
//     timestamp: new Date().toISOString()
//   });
// });

// // Endpoint to initiate connection
// app.post('/api/connect', async (req, res) => {
//   const { sessionId } = req.body;
  
//   console.log('\nðŸ”µ === CONNECT REQUEST ===');
//   console.log('ðŸ“¦ Request body:', req.body);
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   if (!sessionId) {
//     console.log('âŒ No sessionId provided');
//     return res.status(400).json({ error: 'sessionId is required' });
//   }
  
//   try {
//     console.log('ðŸ”Œ Attempting to connect to OpenAI...');
    
//     // Connect to OpenAI Realtime API
//     const openaiWs = new WebSocket(
//       'wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17',
//       {
//         headers: {
//           'Authorization': `Bearer ${OPENAI_API_KEY}`,
//           'OpenAI-Beta': 'realtime=v1'
//         }
//       }
//     );
    
//     openaiWs.on('open', () => {
//       console.log('âœ… Connected to OpenAI for session:', sessionId);
      
//       // Configure session
//       const sessionConfig = {
//         type: 'session.update',
//         session: {
//           modalities: ['text', 'audio'],
//           instructions: 'You are a helpful AI meeting assistant. Be concise, friendly, and natural in conversation. Keep responses under 3 sentences.',
//           voice: 'alloy',
//           input_audio_format: 'pcm16',
//           output_audio_format: 'pcm16',
//           turn_detection: {
//             type: 'server_vad',
//             threshold: 0.5,
//             prefix_padding_ms: 300,
//             silence_duration_ms: 500
//           }
//         }
//       };
      
//       console.log('ðŸ“¤ Sending session config to OpenAI');
//       openaiWs.send(JSON.stringify(sessionConfig));
      
//       openaiConnections.set(sessionId, openaiWs);
//       console.log('ðŸ’¾ Stored connection for session:', sessionId);
//       console.log('ðŸ“Š Total active connections:', openaiConnections.size);
//     });
    
//     // OpenAI -> Store audio locally or send via Pusher
//     openaiWs.on('message', (data) => {
//       try {
//         const message = JSON.parse(data.toString());
//         console.log('ðŸ“¨ Message from OpenAI:', message.type);
        
//         if (message.type === 'session.created') {
//           console.log('ðŸŽ‰ OpenAI session created successfully');
//         }
        
//         if (message.type === 'response.audio.delta') {
//           console.log('ðŸ”Š Audio response received from OpenAI');
          
//           // Store audio locally instead of sending via Pusher
//           if (!audioResponses.has(sessionId)) {
//             audioResponses.set(sessionId, []);
//           }
//           audioResponses.get(sessionId).push(message.delta);
//         }
        
//         if (message.type === 'input_audio_buffer.speech_started') {
//           console.log('ðŸŽ¤ OpenAI detected speech start');
//         }
        
//         if (message.type === 'input_audio_buffer.speech_stopped') {
//           console.log('ðŸŽ¤ OpenAI detected speech stop');
//         }
        
//         // Only send non-audio messages via Pusher
//         if (message.type !== 'response.audio.delta') {
//           const channel = `session-${sessionId}`;
//           pusher.trigger(channel, 'openai-message', message).catch(err => {
//             console.error('âŒ Pusher error:', err.message);
//           });
//         }
        
//       } catch (error) {
//         console.error('âŒ Error parsing OpenAI message:', error);
//       }
//     });
    
//     openaiWs.on('close', () => {
//       console.log('ðŸ”´ OpenAI disconnected for session:', sessionId);
//       openaiConnections.delete(sessionId);
//       audioResponses.delete(sessionId);
//       console.log('ðŸ“Š Remaining connections:', openaiConnections.size);
//     });
    
//     openaiWs.on('error', (error) => {
//       console.error('âŒ OpenAI WebSocket error:', error.message);
//       openaiConnections.delete(sessionId);
//       audioResponses.delete(sessionId);
//     });
    
//     res.json({ success: true, sessionId });
//     console.log('âœ… Connect response sent');
    
//   } catch (error) {
//     console.error('âŒ Connection error:', error);
//     res.status(500).json({ error: error.message });
//   }
// });

// // Endpoint to send audio to OpenAI
// app.post('/api/send-audio', async (req, res) => {
//   const { sessionId, audio } = req.body;
  
//   console.log('\nðŸ”µ === SEND AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
//   console.log('ðŸŽµ Audio data length:', audio ? audio.length : 0);
  
//   if (!sessionId || !audio) {
//     console.log('âŒ Missing required fields');
//     return res.status(400).json({ error: 'sessionId and audio are required' });
//   }
  
//   const openaiWs = openaiConnections.get(sessionId);
  
//   if (!openaiWs) {
//     console.log('âŒ No connection found for session:', sessionId);
//     console.log('ðŸ“Š Active sessions:', Array.from(openaiConnections.keys()));
//     return res.status(400).json({ error: 'No active connection for this session' });
//   }
  
//   console.log('ðŸ” WebSocket state:', openaiWs.readyState, '(1 = OPEN)');
  
//   if (openaiWs.readyState === WebSocket.OPEN) {
//     try {
//       const audioMessage = {
//         type: 'input_audio_buffer.append',
//         audio: audio
//       };
      
//       console.log('ðŸ“¤ Sending audio to OpenAI...');
//       openaiWs.send(JSON.stringify(audioMessage));
//       console.log('âœ… Audio sent successfully');
      
//       res.json({ success: true });
//     } catch (error) {
//       console.error('âŒ Error sending audio:', error);
//       res.status(500).json({ error: 'Failed to send audio' });
//     }
//   } else {
//     console.log('âŒ WebSocket not open. State:', openaiWs.readyState);
//     res.status(400).json({ error: 'Connection not ready' });
//   }
// });

// // Endpoint to get audio responses
// app.get('/api/get-audio/:sessionId', (req, res) => {
//   const { sessionId } = req.params;
  
//   console.log('ðŸ”µ === GET AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   const audioChunks = audioResponses.get(sessionId) || [];
  
//   if (audioChunks.length > 0) {
//     console.log('âœ… Returning', audioChunks.length, 'audio chunks');
//     const chunks = [...audioChunks];
//     audioResponses.set(sessionId, []); // Clear after getting
//     res.json({ audio: chunks });
//   } else {
//     res.json({ audio: [] });
//   }
// });

// // Error handling middleware
// app.use((error, req, res, next) => {
//   console.error('âŒ Server Error:', error);
  
//   res.status(500).json({ 
//     error: 'Internal server error',
//     ...(process.env.NODE_ENV === 'development' && { details: error.message })
//   });
// });

// // 404 handler
// app.use((req, res) => {
//   console.log('âŒ 404 - Route not found:', req.path);
//   res.status(404).json({ error: 'Route not found' });
// });

// // For local development
// const startServer = async () => {
//   const PORT = process.env.PORT || 3000;
//   app.listen(PORT, () => {
//     console.log('\nðŸš€ðŸš€ðŸš€ SERVER STARTED ðŸš€ðŸš€ðŸš€');
//     console.log(`ðŸ“¡ Pusher Channels ready for real-time updates`);
//     console.log(`ðŸŒ API available at: http://localhost:${PORT}`);
//     console.log(`ðŸŽ¯ Environment: ${process.env.NODE_ENV || 'development'}\n`);
//   });
// };

// // Only start server if not in a serverless environment
// if (process.env.NODE_ENV !== 'production' || !process.env.VERCEL) {
//   startServer();
// }

// // Export the Express app for Vercel
// module.exports = app;

// const express = require('express');
// const cors = require('cors');
// const Pusher = require('pusher');
// const WebSocket = require('ws');
// require('dotenv').config();
// const { createClient } = require('@deepgram/sdk');

// const app = express();

// // Initialize Pusher
// const pusher = new Pusher({
//   appId: process.env.PUSHER_APP_ID,
//   key: process.env.PUSHER_KEY,
//   secret: process.env.PUSHER_SECRET,
//   cluster: process.env.PUSHER_CLUSTER,
//   useTLS: true
// });

// console.log('ðŸ”§ Pusher initialized with cluster:', process.env.PUSHER_CLUSTER);

// // Middleware
// app.use(cors({
//   origin: '*',
//   methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
//   allowedHeaders: ['Origin', 'X-Requested-With', 'Content-Type', 'Accept', 'Authorization'],
// }));
// app.use(express.json({ limit: '10mb' }));
// app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
// console.log('ðŸ”‘ OpenAI API Key present:', !!OPENAI_API_KEY);

// const DEEPGRAM_API_KEY = process.env.DEEPGRAM_API_KEY;
// console.log('ðŸ”‘ Deepgram API Key present:', !!DEEPGRAM_API_KEY);

// // Initialize Deepgram client
// const deepgram = createClient(DEEPGRAM_API_KEY);
// console.log('ðŸŽ™ï¸ Deepgram client initialized');

// // Store active Deepgram connections
// const deepgramConnections = new Map();
// // Store transcripts per session
// const transcripts = new Map();
// // Store audio responses per session
// const audioResponses = new Map();

// // Function to process transcript with LLM
// async function processWithLLM(sessionId, userMessage) {
//   try {
//     console.log('ðŸ¤– Processing with LLM:', userMessage);
    
//     // Call OpenAI API
//     const response = await fetch('https://api.openai.com/v1/chat/completions', {
//       method: 'POST',
//       headers: {
//         'Content-Type': 'application/json',
//         'Authorization': `Bearer ${OPENAI_API_KEY}`
//       },
//       body: JSON.stringify({
//         model: 'gpt-4o-mini',
//         messages: [
//           {
//             role: 'system',
//             content: 'You are a helpful AI meeting assistant. Be concise, friendly, and natural in conversation. Keep responses under 3 sentences.'
//           },
//           {
//             role: 'user',
//             content: userMessage
//           }
//         ],
//         max_tokens: 150,
//         temperature: 0.7
//       })
//     });
    
//     const data = await response.json();
//     const aiResponse = data.choices[0].message.content;
    
//     console.log('ðŸ¤– LLM Response:', aiResponse);
    
//     // Send response via Pusher
//     const channel = `session-${sessionId}`;
//     pusher.trigger(channel, 'llm-response', {
//       text: aiResponse
//     }).catch(err => {
//       console.error('âŒ Pusher error:', err.message);
//     });
    
//     // Now convert to speech with Deepgram TTS
//     await convertToSpeech(sessionId, aiResponse);
    
//   } catch (error) {
//     console.error('âŒ Error processing with LLM:', error);
//   }
// }

// // Function to convert text to speech using Deepgram TTS
// async function convertToSpeech(sessionId, text) {
//   try {
//     console.log('ðŸ”Š Converting to speech:', text);
    
//     // Call Deepgram TTS API
//     const response = await deepgram.speak.request(
//       { text },
//       {
//         model: 'aura-asteria-en',
//         encoding: 'linear16',
//         sample_rate: 24000
//       }
//     );
    
//     // Get audio stream
//     const stream = await response.getStream();
//     const audioChunks = [];
    
//     // Collect audio chunks
//     for await (const chunk of stream) {
//       audioChunks.push(chunk);
//     }
    
//     // Combine all chunks
//     const audioBuffer = Buffer.concat(audioChunks);
//     const base64Audio = audioBuffer.toString('base64');
    
//     console.log('âœ… Audio generated, size:', audioBuffer.length);
    
//     // Store audio for client to retrieve
//     if (!audioResponses.has(sessionId)) {
//       audioResponses.set(sessionId, []);
//     }
//     audioResponses.get(sessionId).push(base64Audio);
    
//     console.log('ðŸ’¾ Audio stored for session:', sessionId);
    
//   } catch (error) {
//     console.error('âŒ Error converting to speech:', error);
//   }
// }

// // Routes
// app.get('/', (req, res) => {
//   console.log('ðŸ“ Root endpoint hit');
//   res.json({ 
//     message: 'Zoom Voice Bot API with Deepgram',
//     status: 'running',
//     endpoints: {
//       health: '/api/health',
//       connect: '/api/connect',
//       sendAudio: '/api/send-audio',
//       getAudio: '/api/get-audio/:sessionId',
//       getTranscript: '/api/get-transcript/:sessionId'
//     }
//   });
// });

// app.get('/api/health', (req, res) => {
//   console.log('ðŸ“ Health check endpoint hit');
//   res.json({ 
//     status: 'ok', 
//     connections: deepgramConnections.size,
//     timestamp: new Date().toISOString()
//   });
// });

// // Endpoint to initiate connection with Deepgram
// app.post('/api/connect', async (req, res) => {
//   const { sessionId } = req.body;
  
//   console.log('\nðŸ”µ === CONNECT REQUEST ===');
//   console.log('ðŸ“¦ Request body:', req.body);
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   if (!sessionId) {
//     console.log('âŒ No sessionId provided');
//     return res.status(400).json({ error: 'sessionId is required' });
//   }
  
//   try {
//     console.log('ðŸ”Œ Attempting to connect to Deepgram STT...');
    
//     // Create Deepgram live transcription connection
//     const dgConnection = deepgram.listen.live({
//       model: 'nova-2',
//       language: 'en',
//       smart_format: true,
//       interim_results: true,
//       utterance_end_ms: 1000,
//       vad_events: true,
//       encoding: 'linear16',
//       sample_rate: 24000,
//       channels: 1
//     });
    
//     // Handle connection open
//     dgConnection.on('open', () => {
//       console.log('âœ… Connected to Deepgram STT for session:', sessionId);
//       deepgramConnections.set(sessionId, dgConnection);
//       console.log('ðŸ’¾ Stored Deepgram connection for session:', sessionId);
//       console.log('ðŸ“Š Total active connections:', deepgramConnections.size);
//     });
    
//     // Handle transcription results
//     dgConnection.on('Results', (data) => {
//       const transcript = data.channel.alternatives[0].transcript;
      
//       if (transcript && transcript.length > 0) {
//         console.log('ðŸ“ Transcript:', transcript);
//         console.log('ðŸŽ¯ Is final:', data.is_final);
        
//         // Store transcript
//         if (!transcripts.has(sessionId)) {
//           transcripts.set(sessionId, []);
//         }
        
//         transcripts.get(sessionId).push({
//           text: transcript,
//           is_final: data.is_final,
//           timestamp: new Date().toISOString()
//         });
        
//         // Send transcript via Pusher
//         const channel = `session-${sessionId}`;
//         pusher.trigger(channel, 'transcript', {
//           text: transcript,
//           is_final: data.is_final
//         }).catch(err => {
//           console.error('âŒ Pusher error:', err.message);
//         });
        
//         // If final transcript, send to LLM
//         if (data.is_final) {
//           console.log('âœ… Final transcript received, sending to LLM...');
//           processWithLLM(sessionId, transcript);
//         }
//       }
//     });
    
//     // Handle metadata
//     dgConnection.on('Metadata', (data) => {
//       console.log('ðŸ“Š Deepgram metadata received');
//     });
    
//     // Handle errors
//     dgConnection.on('error', (error) => {
//       console.error('âŒ Deepgram error:', error);
//     });
    
//     // Handle close
//     dgConnection.on('close', () => {
//       console.log('ðŸ”´ Deepgram disconnected for session:', sessionId);
//       deepgramConnections.delete(sessionId);
//       transcripts.delete(sessionId);
//     });
    
//     res.json({ success: true, sessionId, service: 'deepgram' });
//     console.log('âœ… Connect response sent');
    
//   } catch (error) {
//     console.error('âŒ Connection error:', error);
//     res.status(500).json({ error: error.message });
//   }
// });

// // Endpoint to send audio to Deepgram
// app.post('/api/send-audio', async (req, res) => {
//   const { sessionId, audio } = req.body;
  
//   console.log('\nðŸ”µ === SEND AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
//   console.log('ðŸŽµ Audio data length:', audio ? audio.length : 0);
  
//   if (!sessionId || !audio) {
//     console.log('âŒ Missing required fields');
//     return res.status(400).json({ error: 'sessionId and audio are required' });
//   }
  
//   const dgConnection = deepgramConnections.get(sessionId);
  
//   if (!dgConnection) {
//     console.log('âŒ No Deepgram connection found for session:', sessionId);
//     console.log('ðŸ“Š Active sessions:', Array.from(deepgramConnections.keys()));
//     return res.status(400).json({ error: 'No active connection for this session' });
//   }
  
//   try {
//     // Decode base64 audio to buffer
//     const audioBuffer = Buffer.from(audio, 'base64');
    
//     console.log('ðŸ“¤ Sending audio to Deepgram... Buffer size:', audioBuffer.length);
    
//     // Send audio to Deepgram
//     dgConnection.send(audioBuffer);
    
//     console.log('âœ… Audio sent to Deepgram successfully');
    
//     res.json({ success: true });
//   } catch (error) {
//     console.error('âŒ Error sending audio to Deepgram:', error);
//     res.status(500).json({ error: 'Failed to send audio' });
//   }
// });

// // Endpoint to get transcripts
// app.get('/api/get-transcript/:sessionId', (req, res) => {
//   const { sessionId } = req.params;
  
//   console.log('ðŸ”µ === GET TRANSCRIPT REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   const sessionTranscripts = transcripts.get(sessionId) || [];
  
//   if (sessionTranscripts.length > 0) {
//     console.log('âœ… Returning', sessionTranscripts.length, 'transcripts');
//     const texts = [...sessionTranscripts];
//     transcripts.set(sessionId, []); // Clear after getting
//     res.json({ transcripts: texts });
//   } else {
//     res.json({ transcripts: [] });
//   }
// });

// // Endpoint to get audio responses
// app.get('/api/get-audio/:sessionId', (req, res) => {
//   const { sessionId } = req.params;
  
//   console.log('ðŸ”µ === GET AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   const audioChunks = audioResponses.get(sessionId) || [];
  
//   if (audioChunks.length > 0) {
//     console.log('âœ… Returning', audioChunks.length, 'audio chunks');
//     const chunks = [...audioChunks];
//     audioResponses.set(sessionId, []); // Clear after getting
//     res.json({ audio: chunks });
//   } else {
//     res.json({ audio: [] });
//   }
// });

// // Error handling middleware
// app.use((error, req, res, next) => {
//   console.error('âŒ Server Error:', error);
  
//   res.status(500).json({ 
//     error: 'Internal server error',
//     ...(process.env.NODE_ENV === 'development' && { details: error.message })
//   });
// });

// // 404 handler
// app.use((req, res) => {
//   console.log('âŒ 404 - Route not found:', req.path);
//   res.status(404).json({ error: 'Route not found' });
// });

// // For local development
// const startServer = async () => {
//   const PORT = process.env.PORT || 3000;
//   app.listen(PORT, () => {
//     console.log('\nðŸš€ðŸš€ðŸš€ SERVER STARTED ðŸš€ðŸš€ðŸš€');
//     console.log(`ðŸ“¡ Pusher Channels ready for real-time updates`);
//     console.log(`ðŸŽ™ï¸ Deepgram STT ready`);
//     console.log(`ðŸ”Š Deepgram TTS ready`);
//     console.log(`ðŸ¤– OpenAI LLM ready`);
//     console.log(`ðŸŒ API available at: http://localhost:${PORT}`);
//     console.log(`ðŸŽ¯ Environment: ${process.env.NODE_ENV || 'development'}\n`);
//   });
// };

// // Only start server if not in a serverless environment
// if (process.env.NODE_ENV !== 'production' || !process.env.VERCEL) {
//   startServer();
// }

// // Export the Express app for Vercel
// module.exports = app;



// const express = require('express');
// const cors = require('cors');
// const Pusher = require('pusher');
// const WebSocket = require('ws');
// require('dotenv').config();

// const app = express();

// // Initialize Pusher
// const pusher = new Pusher({
//   appId: process.env.PUSHER_APP_ID,
//   key: process.env.PUSHER_KEY,
//   secret: process.env.PUSHER_SECRET,
//   cluster: process.env.PUSHER_CLUSTER,
//   useTLS: true
// });

// console.log('ðŸ”§ Pusher initialized with cluster:', process.env.PUSHER_CLUSTER);

// // Middleware
// app.use(cors({
//   origin: '*',
//   methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
//   allowedHeaders: ['Origin', 'X-Requested-With', 'Content-Type', 'Accept', 'Authorization'],
// }));
// app.use(express.json({ limit: '10mb' }));
// app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
// console.log('ðŸ”‘ OpenAI API Key present:', !!OPENAI_API_KEY);

// // Store active OpenAI connections
// const openaiConnections = new Map();
// // Store audio responses per session
// const audioResponses = new Map();

// // Routes
// app.get('/', (req, res) => {
//   console.log('ðŸ“ Root endpoint hit');
//   res.json({ 
//     message: 'Zoom Voice Bot API',
//     status: 'running',
//     endpoints: {
//       health: '/api/health',
//       connect: '/api/connect',
//       sendAudio: '/api/send-audio',
//       getAudio: '/api/get-audio/:sessionId'
//     }
//   });
// });

// app.get('/api/health', (req, res) => {
//   console.log('ðŸ“ Health check endpoint hit');
//   res.json({ 
//     status: 'ok', 
//     connections: openaiConnections.size,
//     timestamp: new Date().toISOString()
//   });
// });

// // Endpoint to initiate connection
// app.post('/api/connect', async (req, res) => {
//   const { sessionId } = req.body;
  
//   console.log('\nðŸ”µ === CONNECT REQUEST ===');
//   console.log('ðŸ“¦ Request body:', req.body);
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   if (!sessionId) {
//     console.log('âŒ No sessionId provided');
//     return res.status(400).json({ error: 'sessionId is required' });
//   }
  
//   try {
//     console.log('ðŸ”Œ Attempting to connect to OpenAI...');
    
//     // Connect to OpenAI Realtime API
//     const openaiWs = new WebSocket(
//       'wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17',
//       {
//         headers: {
//           'Authorization': `Bearer ${OPENAI_API_KEY}`,
//           'OpenAI-Beta': 'realtime=v1'
//         }
//       }
//     );
    
//     openaiWs.on('open', () => {
//       console.log('âœ… Connected to OpenAI for session:', sessionId);
      
//       // Configure session
//       const sessionConfig = {
//         type: 'session.update',
//         session: {
//           modalities: ['text', 'audio'],
//           instructions: 'You are a helpful AI meeting assistant. Be concise, friendly, and natural in conversation. Keep responses under 3 sentences.',
//           voice: 'alloy',
//           input_audio_format: 'pcm16',
//           output_audio_format: 'pcm16',
//           turn_detection: {
//             type: 'server_vad',
//             threshold: 0.5,
//             prefix_padding_ms: 300,
//             silence_duration_ms: 500
//           }
//         }
//       };
      
//       console.log('ðŸ“¤ Sending session config to OpenAI');
//       openaiWs.send(JSON.stringify(sessionConfig));
      
//       openaiConnections.set(sessionId, openaiWs);
//       console.log('ðŸ’¾ Stored connection for session:', sessionId);
//       console.log('ðŸ“Š Total active connections:', openaiConnections.size);
//     });
    
//     // OpenAI -> Store audio locally or send via Pusher
//     openaiWs.on('message', (data) => {
//       try {
//         const message = JSON.parse(data.toString());
//         console.log('ðŸ“¨ Message from OpenAI:', message.type);
        
//         if (message.type === 'session.created') {
//           console.log('ðŸŽ‰ OpenAI session created successfully');
//         }
        
//         if (message.type === 'response.audio.delta') {
//           console.log('ðŸ”Š Audio response received from OpenAI');
          
//           // Store audio locally instead of sending via Pusher
//           if (!audioResponses.has(sessionId)) {
//             audioResponses.set(sessionId, []);
//           }
//           audioResponses.get(sessionId).push(message.delta);
//         }
        
//         if (message.type === 'input_audio_buffer.speech_started') {
//           console.log('ðŸŽ¤ OpenAI detected speech start');
//         }
        
//         if (message.type === 'input_audio_buffer.speech_stopped') {
//           console.log('ðŸŽ¤ OpenAI detected speech stop');
//         }
        
//         // Only send non-audio messages via Pusher
//         if (message.type !== 'response.audio.delta') {
//           const channel = `session-${sessionId}`;
//           pusher.trigger(channel, 'openai-message', message).catch(err => {
//             console.error('âŒ Pusher error:', err.message);
//           });
//         }
        
//       } catch (error) {
//         console.error('âŒ Error parsing OpenAI message:', error);
//       }
//     });
    
//     openaiWs.on('close', () => {
//       console.log('ðŸ”´ OpenAI disconnected for session:', sessionId);
//       openaiConnections.delete(sessionId);
//       audioResponses.delete(sessionId);
//       console.log('ðŸ“Š Remaining connections:', openaiConnections.size);
//     });
    
//     openaiWs.on('error', (error) => {
//       console.error('âŒ OpenAI WebSocket error:', error.message);
//       openaiConnections.delete(sessionId);
//       audioResponses.delete(sessionId);
//     });
    
//     res.json({ success: true, sessionId });
//     console.log('âœ… Connect response sent');
    
//   } catch (error) {
//     console.error('âŒ Connection error:', error);
//     res.status(500).json({ error: error.message });
//   }
// });

// // Endpoint to send audio to OpenAI
// app.post('/api/send-audio', async (req, res) => {
//   const { sessionId, audio } = req.body;
  
//   console.log('\nðŸ”µ === SEND AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
//   console.log('ðŸŽµ Audio data length:', audio ? audio.length : 0);
  
//   if (!sessionId || !audio) {
//     console.log('âŒ Missing required fields');
//     return res.status(400).json({ error: 'sessionId and audio are required' });
//   }
  
//   const openaiWs = openaiConnections.get(sessionId);
  
//   if (!openaiWs) {
//     console.log('âŒ No connection found for session:', sessionId);
//     console.log('ðŸ“Š Active sessions:', Array.from(openaiConnections.keys()));
//     return res.status(400).json({ error: 'No active connection for this session' });
//   }
  
//   console.log('ðŸ” WebSocket state:', openaiWs.readyState, '(1 = OPEN)');
  
//   if (openaiWs.readyState === WebSocket.OPEN) {
//     try {
//       const audioMessage = {
//         type: 'input_audio_buffer.append',
//         audio: audio
//       };
      
//       console.log('ðŸ“¤ Sending audio to OpenAI...');
//       openaiWs.send(JSON.stringify(audioMessage));
//       console.log('âœ… Audio sent successfully');
      
//       res.json({ success: true });
//     } catch (error) {
//       console.error('âŒ Error sending audio:', error);
//       res.status(500).json({ error: 'Failed to send audio' });
//     }
//   } else {
//     console.log('âŒ WebSocket not open. State:', openaiWs.readyState);
//     res.status(400).json({ error: 'Connection not ready' });
//   }
// });

// // Endpoint to get audio responses
// app.get('/api/get-audio/:sessionId', (req, res) => {
//   const { sessionId } = req.params;
  
//   console.log('ðŸ”µ === GET AUDIO REQUEST ===');
//   console.log('ðŸ†” Session ID:', sessionId);
  
//   const audioChunks = audioResponses.get(sessionId) || [];
  
//   if (audioChunks.length > 0) {
//     console.log('âœ… Returning', audioChunks.length, 'audio chunks');
//     const chunks = [...audioChunks];
//     audioResponses.set(sessionId, []); // Clear after getting
//     res.json({ audio: chunks });
//   } else {
//     res.json({ audio: [] });
//   }
// });

// // Error handling middleware
// app.use((error, req, res, next) => {
//   console.error('âŒ Server Error:', error);
  
//   res.status(500).json({ 
//     error: 'Internal server error',
//     ...(process.env.NODE_ENV === 'development' && { details: error.message })
//   });
// });

// // 404 handler
// app.use((req, res) => {
//   console.log('âŒ 404 - Route not found:', req.path);
//   res.status(404).json({ error: 'Route not found' });
// });

// // For local development
// const startServer = async () => {
//   const PORT = process.env.PORT || 3000;
//   app.listen(PORT, () => {
//     console.log('\nðŸš€ðŸš€ðŸš€ SERVER STARTED ðŸš€ðŸš€ðŸš€');
//     console.log(`ðŸ“¡ Pusher Channels ready for real-time updates`);
//     console.log(`ðŸŒ API available at: http://localhost:${PORT}`);
//     console.log(`ðŸŽ¯ Environment: ${process.env.NODE_ENV || 'development'}\n`);
//   });
// };

// // Only start server if not in a serverless environment
// if (process.env.NODE_ENV !== 'production' || !process.env.VERCEL) {
//   startServer();
// }

// // Export the Express app for Vercel
// module.exports = app;
const express = require('express');
const cors = require('cors');
const Pusher = require('pusher');
const WebSocket = require('ws');
require('dotenv').config();
const { createClient } = require('@deepgram/sdk');

const app = express();

const pusher = new Pusher({
  appId: process.env.PUSHER_APP_ID,
  key: process.env.PUSHER_KEY,
  secret: process.env.PUSHER_SECRET,
  cluster: process.env.PUSHER_CLUSTER,
  useTLS: true
});

console.log('ðŸ”§ Pusher initialized');

// CORS
app.use(cors({
  origin: '*',
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Origin', 'X-Requested-With', 'Content-Type', 'Accept', 'Authorization'],
}));

app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const DEEPGRAM_API_KEY = process.env.DEEPGRAM_API_KEY;

console.log('ðŸ”‘ OpenAI API Key present:', !!OPENAI_API_KEY);
console.log('ðŸ”‘ Deepgram API Key present:', !!DEEPGRAM_API_KEY);

const deepgram = createClient(DEEPGRAM_API_KEY);
console.log('ðŸŽ™ï¸ Deepgram client initialized');

const deepgramConnections = new Map();
const audioResponses = new Map();
const conversationHistory = new Map();

async function processWithLLM(sessionId, userMessage, t0) {
  try {
    if (!conversationHistory.has(sessionId)) {
      conversationHistory.set(sessionId, []);
    }
    const history = conversationHistory.get(sessionId);
    
    history.push({ role: 'user', content: userMessage });
    if (history.length > 8) history.splice(0, 2);
    
    const t_llm_start = Date.now();
    console.log(`[${t_llm_start - t0}ms] LLM START`);
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'Reply in 1 short sentence (8-12 words). Be natural and conversational.'
          },
          ...history
        ],
        max_tokens: 30,
        temperature: 0.7,
        stream: false
      })
    });
    
    const data = await response.json();
    const fullResponse = data.choices[0].message.content.trim();
    
    const t_llm_end = Date.now();
    console.log(`[${t_llm_end - t0}ms] LLM END: "${fullResponse}"`);
    
    // Send AI response to frontend via Pusher
    const channel = `session-${sessionId}`;
    
    try {
      await pusher.trigger(channel, 'ai-response', {
        text: fullResponse
      });
      console.log(`âœ… AI response sent via Pusher`);
    } catch (err) {
      console.error('âŒ Pusher error:', err);
    }
    
    history.push({ role: 'assistant', content: fullResponse });
    
    await convertToSpeech(sessionId, fullResponse, t0);
    
  } catch (error) {
    console.error('LLM ERROR:', error.message);
  }
}

async function convertToSpeech(sessionId, text, t0) {
  try {
    const t_tts_start = Date.now();
    console.log(`[${t_tts_start - t0}ms] TTS START`);
    
    const response = await deepgram.speak.request(
      { text },
      {
        model: 'aura-asteria-en',
        encoding: 'linear16',
        sample_rate: 24000,
        container: 'none'
      }
    );
    
    const stream = await response.getStream();
    const audioChunks = [];
    
    for await (const chunk of stream) {
      audioChunks.push(chunk);
    }
    
    const audioBuffer = Buffer.concat(audioChunks);
    const base64Audio = audioBuffer.toString('base64');
    
    const t_tts_end = Date.now();
    console.log(`[${t_tts_end - t0}ms] TTS END`);
    
    // Send notification that audio was received
    const channel = `session-${sessionId}`;
    
    try {
      await pusher.trigger(channel, 'audio-received', {
        message: 'Received audio from Deepgram',
        timestamp: Date.now()
      });
      console.log(`âœ… Audio-received notification sent`);
    } catch (err) {
      console.error('âŒ Pusher error:', err);
    }
    
    if (!audioResponses.has(sessionId)) {
      audioResponses.set(sessionId, []);
    }
    audioResponses.get(sessionId).push({ audio: base64Audio, t0: t0 });
    
  } catch (error) {
    console.error('TTS ERROR:', error.message);
  }
}

app.get('/', (req, res) => {
  res.json({ 
    message: 'Zoom Voice Bot', 
    status: 'running',
    endpoints: {
      health: '/api/health',
      connect: '/api/connect',
      sendAudio: '/api/send-audio',
      getAudio: '/api/get-audio/:sessionId'
    }
  });
});

app.get('/api/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    connections: deepgramConnections.size,
    timestamp: new Date().toISOString()
  });
});

app.post('/api/connect', async (req, res) => {
  const { sessionId } = req.body;
  
  console.log('\nðŸ”µ CONNECT:', sessionId);
  
  if (!sessionId) {
    return res.status(400).json({ error: 'sessionId required' });
  }
  
  try {
    console.log('ðŸ”Œ Connecting to Deepgram STT...');
    
    const dgConnection = deepgram.listen.live({
      model: 'nova-2',
      language: 'en',
      smart_format: true,
      interim_results: true,
      utterance_end_ms: 1000,
      vad_events: true,
      encoding: 'linear16',
      sample_rate: 24000,
      channels: 1
    });
    
    let lastTranscript = '';
    let processingTimer = null;
    let lastProcessedTranscript = '';
    let t0 = null;
    
    dgConnection.on('open', () => {
      console.log(`âœ… Connected: ${sessionId}`);
      deepgramConnections.set(sessionId, dgConnection);
      console.log('ðŸ“Š Total connections:', deepgramConnections.size);
    });
    
    dgConnection.on('Results', (data) => {
      const transcript = data.channel.alternatives[0].transcript;
      
      if (transcript && transcript.length > 0) {
        lastTranscript = transcript;
        
        // Send interim transcripts to frontend
        const channel = `session-${sessionId}`;
        pusher.trigger(channel, 'transcript-interim', {
          text: transcript,
          is_final: data.is_final
        }).catch(err => console.error('Pusher error:', err));
        
        if (processingTimer) {
          clearTimeout(processingTimer);
        }
        
        // Only process final transcripts
        if (data.is_final) {
          processingTimer = setTimeout(() => {
            if (lastTranscript && lastTranscript !== lastProcessedTranscript) {
              t0 = Date.now();
              
              console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
              console.log(`[T=0] USER STOPPED SPEAKING`);
              console.log(`Transcript: "${lastTranscript}"`);
              
              const t_stt_end = Date.now();
              console.log(`[${t_stt_end - t0}ms] STT END`);
              
              // Send final transcript to frontend
              pusher.trigger(channel, 'transcript', {
                text: lastTranscript
              }).then(() => {
                console.log(`âœ… Transcript sent via Pusher`);
              }).catch(err => {
                console.error('âŒ Pusher error:', err);
              });
              
              lastProcessedTranscript = lastTranscript;
              processWithLLM(sessionId, lastTranscript, t0);
            }
          }, 500);
        }
      }
    });
    
    dgConnection.on('error', (error) => {
      console.error('STT ERROR:', error.message);
    });
    
    dgConnection.on('close', () => {
      console.log(`ðŸ”´ Disconnected: ${sessionId}`);
      deepgramConnections.delete(sessionId);
      conversationHistory.delete(sessionId);
      if (processingTimer) clearTimeout(processingTimer);
    });
    
    res.json({ success: true, sessionId, service: 'deepgram' });
    console.log('âœ… Connect response sent');
    
  } catch (error) {
    console.error('CONNECT ERROR:', error);
    res.status(500).json({ error: error.message });
  }
});

app.post('/api/send-audio', async (req, res) => {
  const { sessionId, audio } = req.body;
  
  if (!sessionId || !audio) {
    return res.status(400).json({ error: 'Missing data' });
  }
  
  const dgConnection = deepgramConnections.get(sessionId);
  
  if (!dgConnection) {
    console.log('âŒ No connection for session:', sessionId);
    console.log('ðŸ“Š Active sessions:', Array.from(deepgramConnections.keys()));
    return res.status(400).json({ error: 'No active connection' });
  }
  
  try {
    const audioBuffer = Buffer.from(audio, 'base64');
    dgConnection.send(audioBuffer);
    res.json({ success: true });
  } catch (error) {
    console.error('âŒ Send audio error:', error);
    res.status(500).json({ error: 'Send failed' });
  }
});

app.get('/api/get-audio/:sessionId', (req, res) => {
  const { sessionId } = req.params;
  const audioData = audioResponses.get(sessionId) || [];
  
  if (audioData.length > 0) {
    const data = [...audioData];
    audioResponses.set(sessionId, []);
    res.json({ 
      audio: data.map(d => d.audio),
      t0: data[0].t0
    });
  } else {
    res.json({ audio: [] });
  }
});

app.use((error, req, res, next) => {
  console.error('SERVER ERROR:', error.message);
  res.status(500).json({ error: 'Server error' });
});

app.use((req, res) => {
  res.status(404).json({ error: 'Not found' });
});

const startServer = async () => {
  const PORT = process.env.PORT || 3000;
  app.listen(PORT, () => {
    console.log(`\nâš¡ Server running on http://localhost:${PORT}\n`);
  });
};

if (process.env.NODE_ENV !== 'production' || !process.env.VERCEL) {
  startServer();
}

module.exports = app;